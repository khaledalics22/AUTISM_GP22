{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import imageio as io\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import normalize,StandardScaler\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import skimage.io as io\n",
    "from skimage.feature import local_binary_pattern\n",
    "from skimage import color\n",
    "from operator import eq\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_data = pd.read_csv('fer2013.csv ')\n",
    "emotion_data_new = pd.read_csv('fer2013new.csv')\n",
    "X_train = []\n",
    "y_train = []\n",
    "X_test = []\n",
    "y_test = []\n",
    "emotions=['neutral','happiness','surprise','sadness','anger','disgust','fear','contempt']\n",
    "for (index, row), (_, row_n) in zip(emotion_data.iterrows(),emotion_data_new.iterrows()):\n",
    "#     print(row['Usage'])\n",
    "    k = row['pixels'].split(\" \")\n",
    "    if row_n['Usage'] == 'Training':\n",
    "          for j in range(len(emotions)):\n",
    "                if(row_n[emotions[j]]>=5):\n",
    "                    X_train.append(np.array(k))\n",
    "                    y_train.append(j)\n",
    "    elif row_n['Usage'] == 'PublicTest':\n",
    "        for j in range(len(emotions)):\n",
    "            if(row_n[emotions[j]]>=5):\n",
    "                X_test.append(np.array(k))\n",
    "                y_test.append(j)\n",
    "for (index, row), (_, row_n) in zip(emotion_data.iterrows(),emotion_data_new.iterrows()):\n",
    "    k = row['pixels'].split(\" \")\n",
    "    if row_n['Usage'] == 'Training':\n",
    "        for j in range(len(emotions)):\n",
    "            if(row_n[emotions[j]]>=5):\n",
    "                X_train.append(np.array(k))\n",
    "                y_train.append(j)\n",
    "    elif row_n['Usage'] == 'PublicTest':\n",
    "        X_test.append(np.array(k))\n",
    "        y_emotions=[]\n",
    "        for j in range(len(emotions)):\n",
    "            if(row_n[emotions[j]]!=0):\n",
    "                y_emotions.append(j)\n",
    "        y_test.append(y_emotions)\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)\n",
    "print(X_test.shape)\n",
    "\n",
    "#################################################\n",
    "# X_train =np.reshape(X_train,(147, 350))\n",
    "# X_test =np.reshape(X_test,(147, 350))\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0],48, 48, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0],48, 48, 1)\n",
    "X_train=X_train.reshape((len(X_train),48,48))\n",
    "X_test=X_test.reshape((len(X_test),48,48))\n",
    "################################################\n",
    "X_test = X_test.astype(np.float)\n",
    "#y_test = y_test.astype(np.integer)\n",
    "X_train = X_train.astype(np.float)\n",
    "y_train = y_train.astype(np.integer)        \n",
    "test_labels=y_test\n",
    "train_labels=y_train\n",
    "train_features=[feature_extract(img,8) for img in X_train]\n",
    "train_features=np.array(train_features)\n",
    "\n",
    "test_features=[feature_extract(img,8) for img in X_test]\n",
    "test_features=np.array(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def CodeToValue(code,count):\n",
    "    binary=[1,2,4,8,16,32,64,128]\n",
    "    codetovalue=0\n",
    "    for i in range(count-1):\n",
    "        codetovalue=codetovalue+code[i]*binary[i]\n",
    "    return codetovalue    \n",
    "\n",
    "def bottomlbp(block,x,y):\n",
    "    bottomdlbp=[0,0,0,0]\n",
    "    four=block[x-1][y]\n",
    "    six=block[x+1][y]\n",
    "    seven=block[x-1][y-1]\n",
    "    eight=block[x][y-1]\n",
    "    nine=block[x+1][y-1]\n",
    "\n",
    "    if(four>=eight):\n",
    "        bottomdlbp[0]=1\n",
    "    if(seven>=eight):\n",
    "        bottomdlbp[1]=1\n",
    "    if(nine>=eight):\n",
    "        bottomdlbp[2]=1\n",
    "    if(six>=eight):\n",
    "        bottomdlbp[3]=1        \n",
    "    return  bottomdlbp   \n",
    "\n",
    "\n",
    "def toplbp(block,x,y):\n",
    "    topdlbp=[0,0,0]\n",
    "    one=block[x-1][y+1]\n",
    "    two=block[x][y+1]\n",
    "    three=block[x+1][y+1]\n",
    "    five=block[x][y]\n",
    "    \n",
    "    if(one>=two):\n",
    "        topdlbp[0]=1\n",
    "    if(five>=two):\n",
    "        topdlbp[1]=1\n",
    "    if(three>=two):\n",
    "        topdlbp[2]=1\n",
    "    return  topdlbp     \n",
    "\n",
    "def histogram(block,size):\n",
    "    hist,bins =np.histogram(block, bins=size, range=(0, size))\n",
    "    return hist,bins\n",
    "\n",
    "def DoubleLocalBinaryPatternBlock(block):\n",
    "\n",
    "    tophist=[]\n",
    "    bottomhist=[]\n",
    "    block=padding(block,1)\n",
    "    w,l=block.shape\n",
    "    blockcopy = np.zeros((w,l), np.uint8)\n",
    "\n",
    "    \n",
    "    for y in range(w-1):\n",
    "        for x in range(l-1):\n",
    "\n",
    "            tdlbpcode=toplbp(block,x,y)\n",
    "            bdlbpcode=bottomlbp(block,x,y)\n",
    "            tdlbpvalue=CodeToValue(tdlbpcode,3)\n",
    "            bdlbpvalue=CodeToValue(bdlbpcode,4)\n",
    "            \n",
    "            tophist.append(tdlbpvalue)\n",
    "            bottomhist.append(bdlbpvalue)\n",
    "#     print(np.array(tophist)  )\n",
    "    return np.array(tophist) ,np.array(bottomhist) \n",
    "\n",
    "def DoubleLocalBinaryPattern(img,block_size):\n",
    "    w,l=img.shape\n",
    "    window=w//block_size\n",
    "    tophistlist=[]\n",
    "    bottomhistlist=[]\n",
    "    for r in range(0,w,window):\n",
    "        for c in range(0,l,window):\n",
    "            block = img[r:r+window,c:c+window]\n",
    "            tophist,bottomhist=DoubleLocalBinaryPatternBlock(block)\n",
    "            th,_=histogram(tophist,8)\n",
    "            bh,_=histogram(bottomhist,16)\n",
    "            tophistlist.append(np.array(th))\n",
    "            bottomhistlist.append(np.array(bh))\n",
    "    feature=[]\n",
    "#     feature.append(tophistlist)\n",
    "#     feature.append(bottomhistlist)\n",
    "#     print(np.array(tophistlist))\n",
    "    feature=np.concatenate((np.array(tophistlist), np.array(bottomhistlist)), axis=None)\n",
    "    return np.array(feature)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the local binary pattern histogram\n",
    "def local(image):\n",
    "    lbp = local_binary_pattern(image, 24,3, method=\"uniform\")\n",
    "    (hist, _) = np.histogram(lbp.ravel(),bins=np.arange(0, 256),range=(0, 256))\n",
    "    hist = hist.astype(\"float\")\n",
    "    return hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate whether the gc greater than alpha set it by 1 otherwise set by -1\n",
    "def f1_alpha(gc,alpha1):\n",
    "    if((gc-alpha1)>=0):\n",
    "        return 1\n",
    "    else:\n",
    "        return -1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate whether the (gc-alpha1)*(gc-alpha2) greater than zero set it by 1 otherwise set by -1\n",
    "def f2_alpha(gc,alpha1,alpha2):\n",
    "    if(((gc-alpha1)*(gc-alpha2)) >=0):\n",
    "        return 1\n",
    "    else:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the pixels that lie in level1\n",
    "def level1(gc,block,x,y):\n",
    "    l1=np.zeros((8,), np.uint8)\n",
    "    l1=[block[x-1][y+1],block[x][y+1],block[x+1][y+1],block[x-1][y],block[x+1][y],block[x-1][y-1],block[x][y-1],block[x+1][y-1]]\n",
    "    return l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the pixels that lie in level2\n",
    "def level2(gc,block,x,y):\n",
    "    l2=np.zeros((16,1), np.uint8)\n",
    "    l2=[   block[x-2][y+2],block[x][y+1],block[x+1][y+1],block[x][y+1],  block[x+2][y+2],\n",
    "           block[x-2][y+1],                                              block[x+2][y+1],\n",
    "           block[x-2][y],                                                block[x+2][y],\n",
    "           block[x-2][y-1],                                              block[x+2][y-1],\n",
    "           block[x-2][y-2],block[x-1][y-2],block[x][y-2],block[x+1][y-2],block[x+2][y-2]\n",
    "        ]\n",
    "\n",
    "    return l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the Taylor Feature Map : when each pixel mapped to Taylor domain , we could tell that we get the TFM\n",
    "def TFM(gc,block,x,y):\n",
    "#first level\n",
    "    lev1=level1(gc,block,x,y)\n",
    "#second level\n",
    "    lev2=level2(gc,block,x,y)\n",
    "#the mean gray value of all the pixels in the second-order TU2\n",
    "#     f0=np.mean(np.array(block).flatten())\n",
    "#the mean gray value of all the pixels in the first layer        \n",
    "    alpha1=np.mean(lev1)\n",
    "#the mean gray value of all the pixels in the second layer        \n",
    "    alpha2=np.mean(lev2)\n",
    "#the mean gray value of all the pixels in the first layer and the second layer \n",
    "    f0=alpha=np.mean(np.concatenate((np.array(lev1), np.array(lev2)), axis=None))\n",
    "#The second-order pixel feature    \n",
    "    f2_gc=f0+f1_alpha(gc,alpha)*(gc-alpha)+f2_alpha(gc,alpha1,alpha2)*((gc-alpha)**2)\n",
    "    return f2_gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Divides the image into NXN blocks , then calculate the TFM for each block and return this TFM block\n",
    "def TFM_i(img,block_size):\n",
    "    w,l=img.shape\n",
    "    window=w//block_size\n",
    "    TFP_hist=[]\n",
    "    tfmcopy = np.zeros((w,l), np.uint8)\n",
    "    for r in range(0,w-2,1):\n",
    "        for c in range(0,l-2,1):\n",
    "            gc=img[c][r]\n",
    "            tfmcopy[c][r]=TFM(gc,img,c,r)\n",
    "    return tfmcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the histogram of every block and then concatenate all these histograms and get our feature vectors\n",
    "def TFM_hist(img,block_size):\n",
    "    w,l=img.shape\n",
    "    window=w//block_size\n",
    "    TFP_hist=[]\n",
    "    img=padding(img,2)\n",
    "    for r in range(2,w-2,window):\n",
    "        for c in range(2,l-2,window):\n",
    "            block = img[r:r+window,c:c+window]\n",
    "            tfmh=local(block)\n",
    "            TFP_hist.append(np.array(tfmh))            \n",
    "    feature=np.array(TFP_hist).flatten()\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is our pipline, first we get the TFM then get the histogram\n",
    "def feature_extract(img,blocksize):\n",
    "    t=TFM_i(img,blocksize)\n",
    "    ht=TFM_hist(t,blocksize)\n",
    "    return ht"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pad input block to specific size\n",
    "def padding(block,size):\n",
    "    padd=np.pad(block, (size, size), 'constant', constant_values=(0, 0))\n",
    "    return padd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the histogram \n",
    "def histogram(block,size):\n",
    "    hist,bins =np.histogram(block, bins=size, range=(0, size))\n",
    "    return hist,bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the accuracy \n",
    "def acc(y_pred,test_labels):\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    yt = le.fit_transform(y_pred)\n",
    "    rt = le.fit_transform(test_labels)\n",
    "    eqw = sum(map(eq, list(rt), list(yt)))\n",
    "    size=len(rt)\n",
    "    acc=(eqw/size)*100\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(feature_set='tse'):\n",
    "    features = []\n",
    "    labels = []\n",
    "    for i, fn in enumerate(img_filenames):\n",
    "            label = fn.split('.')[0]\n",
    "            #labels.append(label)\n",
    "            path = os.path.join(path_to_dataset, fn)\n",
    "            subpath=os.listdir(path)\n",
    "            #print(subpath)\n",
    "            for k, d in enumerate(subpath):             \n",
    "                labels.append(label)\n",
    "                imgpath = os.path.join(path, d)\n",
    "                img = io.imread(imgpath)\n",
    "                resized = cv2.resize(img, (48,48), interpolation = cv2.INTER_AREA)\n",
    "                gray=color.rgb2gray(resized)\n",
    "                logI=Log_Transformation(gray)\n",
    "                lapI=laplacian_Transformation(logI)\n",
    "                features.append(np.array(feature_extract(lapI,16)))\n",
    "                if k > 0 and k % 10== 0:\n",
    "                    print(\"[INFO] processed {}/{}\".format(k, len(subpath)))\n",
    "    return features, labels            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(img, feature_set='lbp'):\n",
    "    if feature_set=='lbp':\n",
    "        return DLBP(img) \n",
    "    if feature_set=='tse':\n",
    "        return Taylor_Exp(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(features,labels,precentage):\n",
    "    train_features, test_features, train_labels, test_labels = train_test_split(\n",
    "        features, labels, test_size=0.2, random_state=random_seed)\n",
    "    return train_features, test_features, train_labels, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalization(train_features, test_features):\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(train_features)\n",
    "\n",
    "    train_features = scaler.transform(train_features)\n",
    "    test_features = scaler.transform(test_features)\n",
    "    return train_features, test_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KNN_classifier(kcount,train_features, train_labels):\n",
    "    classifier = KNeighborsClassifier(n_neighbors=kcount)\n",
    "    classifier.fit(list(train_features),list(train_labels) )\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_classifier(test_features, test_labels,classifier):\n",
    "        accuracy = classifier.score(test_features, test_labels)        \n",
    "        print('accuracy:', accuracy*100, '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Log_Transformation(image):\n",
    "    c = 255 / np.log(1 + np.max(image))+1 \n",
    "    s = c * (np.log(image + 1)) \n",
    "    s = np.array(s, dtype = np.uint8)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def laplacian_Transformation(image):\n",
    "    im = cv2.Laplacian(image,cv2.CV_64F)\n",
    "    return im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_to_dataset =r'E:\\datasetemotion\\val\\val'\n",
    "# path_to_dataset =r'E:\\datasetemotion\\train\\train'\n",
    "# path_to_dataset =r'D:\\Download\\jaffedbase'\n",
    "path_to_dataset =r'E:\\emotion\\archive\\ck\\CK+48'\n",
    "img_filenames = os.listdir(path_to_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['anger', 'contempt', 'disgust', 'fear', 'happy', 'sadness', 'surprise']\n"
     ]
    }
   ],
   "source": [
    "img_filenames = os.listdir(path_to_dataset)\n",
    "print(img_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 0  \n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset. This will take time ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:3: RuntimeWarning: divide by zero encountered in log\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] processed 10/135\n",
      "[INFO] processed 20/135\n",
      "[INFO] processed 30/135\n",
      "[INFO] processed 40/135\n",
      "[INFO] processed 50/135\n",
      "[INFO] processed 60/135\n",
      "[INFO] processed 70/135\n",
      "[INFO] processed 80/135\n",
      "[INFO] processed 90/135\n",
      "[INFO] processed 100/135\n",
      "[INFO] processed 110/135\n",
      "[INFO] processed 120/135\n",
      "[INFO] processed 130/135\n"
     ]
    }
   ],
   "source": [
    "print('Loading dataset. This will take time ...')\n",
    "features, labels = load_dataset(feature_set='tse')\n",
    "print('Finished loading dataset.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features1, labels1=features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, test_features, train_labels, test_labels=split_data(np.array(features1),np.array(labels1),0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=0.7)\n",
    "X_train1 = pca.fit_transform(train_features)\n",
    "X_test1 = pca.transform(test_features)\n",
    "# n_components=1000 must be between 0 and min(n_samples, n_features)=900 with svd_solver='full'\n",
    "train_features=X_train1\n",
    "test_features=X_test1\n",
    "train_labels=train_labels\n",
    "test_labels=test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier=KNN_classifier(3,train_features, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = classifier.score(test_features, test_labels)\n",
    "print('Test accuracy:', accuracy*100, '%')\n",
    "accuracyt = classifier.score(train_features, train_labels)\n",
    "print('Train accuracy:', accuracyt*100, '%') \n",
    "\n",
    "# Test accuracy: 45.17766497461929 %\n",
    "# Train accuracy: 71.68367346938776 %\n",
    "# Test accuracy: 56.852791878172596 %\n",
    "# Train accuracy: 84.0561224489796 %\n",
    "#without LL\n",
    "# Test accuracy: 59.390862944162436 %\n",
    "# Train accuracy: 85.96938775510205 %\n",
    "#with LL\n",
    "# Test accuracy: 58.37563451776649 %\n",
    "# Train accuracy: 86.86224489795919 %\n",
    "# block size =16\n",
    "# Test accuracy: 68.02030456852792 %\n",
    "# Train accuracy: 94.13265306122449 %\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "le = preprocessing.LabelEncoder()\n",
    "random_seed = 0  \n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "sigma=0.0008\n",
    "c=64\n",
    "reg=100\n",
    "err=1e-4\n",
    "def kernel(X, Y=None,g=None):\n",
    "    k=rbf_kernel(X, Y, gamma=g)\n",
    "    return k\n",
    "def KELM_Beta(sigma,c,x,y):\n",
    "    le.fit(y)\n",
    "    y = le.transform(y)\n",
    "    emotion = len(np.unique(y))\n",
    "    One_Hot_Encoding=np.zeros((emotion,emotion), np.uint8)\n",
    "    for em in range (emotion):\n",
    "        One_Hot_Encoding[em][em]=1\n",
    "    T = One_Hot_Encoding[y, :]    \n",
    "    N,d=x.shape\n",
    "    Omega=kernel(x,None,sigma)\n",
    "    I=np.eye(N)\n",
    "    Beta = np.linalg.inv((I /c) + Omega).dot(T)    \n",
    "    return Beta,One_Hot_Encoding\n",
    "def KELM_Output(sigma,x,x_test,Beta,One_Hot_Encoding):\n",
    "    hx=kernel(x_test,x,sigma)\n",
    "    fx= hx.dot(Beta)\n",
    "#     print(np.array(fx).shape)\n",
    "    p=np.argmax(fx,axis=1)\n",
    "    return p\n",
    "def acc(y_pred,test_labels):\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    yt = le.fit_transform(y_pred)    \n",
    "    rt = le.fit_transform(test_labels)\n",
    "    eqw = sum(map(eq, list(rt), list(y_pred)))\n",
    "    size=len(rt)\n",
    "    acc=(eqw/size)*100\n",
    "    return acc\n",
    "Beta,One_Hot_Encoding=KELM_Beta(sigma,c,np.array(train_features),np.array(train_labels))\n",
    "p1=KELM_Output(sigma,np.array(train_features),np.array(test_features),Beta,One_Hot_Encoding)\n",
    "print(\"test\",acc(p1,test_labels))\n",
    "\n",
    "p2=KELM_Output(sigma,np.array(train_features),np.array(train_features),Beta,One_Hot_Encoding)\n",
    "print(\"train\",acc(p2,train_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy_score(test_labels, y_pred))\n",
    "print(classification_report(test_labels, y_pred))\n",
    "print(confusion_matrix(test_labels, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "# k in ['rbf','sigmoid']\\\n",
    "# gamma in ['scale','auto']\n",
    "\n",
    "k='rbf'\n",
    "gamma = 'scale'\n",
    "c=900\n",
    "clf = SVC(C=c,kernel=k,gamma=gamma)\n",
    "clf.fit(train_features, train_labels)\n",
    "\n",
    "print ('Test score', clf.score(test_features, test_labels)*100 )\n",
    "print ('Train score', clf.score(train_features, train_labels)*100 )\n",
    "\n",
    "\n",
    "\n",
    "# Test score 43.14720812182741\n",
    "# Train score 52.67857142857143\n",
    "#withoutLL\n",
    "# Test score 55.32994923857868\n",
    "# Train score 100.0\n",
    "#with LL\n",
    "# Test score 59.390862944162436\n",
    "# Train score 100.0\n",
    "# block size =16\n",
    "# Test score 69.03553299492386\n",
    "# Train score 100.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(197, 7)\n",
      "test 71.57360406091371\n",
      "(784, 7)\n",
      "train 100.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "le = preprocessing.LabelEncoder()\n",
    "random_seed = 0  \n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "sigma=0.0008\n",
    "c=64\n",
    "reg=100\n",
    "err=1e-4\n",
    "\n",
    "def kernel(X, Y=None,g=None):\n",
    "    k=rbf_kernel(X, Y, gamma=g)\n",
    "    return k\n",
    "\n",
    "def KELM_Beta(sigma,c,x,y):\n",
    "    le.fit(y)\n",
    "    y = le.transform(y)\n",
    "    emotion = len(np.unique(y))\n",
    "    One_Hot_Encoding=np.zeros((emotion,emotion), np.uint8)\n",
    "    for em in range (emotion):\n",
    "        One_Hot_Encoding[em][em]=1\n",
    "    T = One_Hot_Encoding[y, :]    \n",
    "    N,d=x.shape\n",
    "    Omega=kernel(x,None,sigma)\n",
    "    I=np.eye(N)\n",
    "    Beta = np.linalg.inv((I /c) + Omega).dot(T)    \n",
    "    return Beta,One_Hot_Encoding\n",
    "\n",
    "def KELM_Output(sigma,x,x_test,Beta,One_Hot_Encoding):\n",
    "    hx=kernel(x_test,x,sigma)\n",
    "    fx= hx.dot(Beta)\n",
    "    print(np.array(fx).shape)\n",
    "    p=np.argmax(fx,axis=1)\n",
    "    return p\n",
    "\n",
    "def acc(y_pred,test_labels):\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    yt = le.fit_transform(y_pred)    \n",
    "    rt = le.fit_transform(test_labels)\n",
    "    eqw = sum(map(eq, list(rt), list(y_pred)))\n",
    "    size=len(rt)\n",
    "    acc=(eqw/size)*100\n",
    "    return acc\n",
    "\n",
    "Beta,One_Hot_Encoding=KELM_Beta(sigma,c,np.array(train_features),np.array(train_labels))\n",
    "p=KELM_Output(sigma,np.array(train_features),np.array(test_features),Beta,One_Hot_Encoding)\n",
    "print(\"test\",acc(p,test_labels))\n",
    "\n",
    "p=KELM_Output(sigma,np.array(train_features),np.array(train_features),Beta,One_Hot_Encoding)\n",
    "print(\"train\",acc(p,train_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=clf.predict(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5939086294416244\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.40      0.60      0.48        20\n",
      "    contempt       0.45      0.83      0.59         6\n",
      "     disgust       0.61      0.41      0.49        41\n",
      "        fear       0.50      0.27      0.35        15\n",
      "       happy       0.62      0.79      0.70        39\n",
      "     sadness       0.50      0.14      0.21        22\n",
      "    surprise       0.70      0.83      0.76        54\n",
      "\n",
      "    accuracy                           0.59       197\n",
      "   macro avg       0.54      0.55      0.51       197\n",
      "weighted avg       0.59      0.59      0.57       197\n",
      "\n",
      "[[12  0  1  1  3  1  2]\n",
      " [ 0  5  0  1  0  0  0]\n",
      " [ 4  2 17  1  9  0  8]\n",
      " [ 3  0  0  4  1  2  5]\n",
      " [ 2  1  4  0 31  0  1]\n",
      " [ 8  2  1  1  4  3  3]\n",
      " [ 1  1  5  0  2  0 45]]\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(test_labels, y))\n",
    "print(classification_report(test_labels, y))\n",
    "print(confusion_matrix(test_labels, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8. 3. 0. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# /happy\n",
    "# img = io.imread('MK.HA1.116.tiff')\n",
    "# /angry\n",
    "# img = io.imread('KA.AN3.41.tiff')\n",
    "# /sad\n",
    "# img = io.imread('KA.SA1.33.tiff')\n",
    "img = io.imread('S010_006_00000013.png')\n",
    "\n",
    "# img  = io.imread('KA.AN2.40.tiff')\n",
    "# img  = io.imread('MK.HA1.116.tiff')\n",
    "# img  = io.imread('UY.DI2.150.tiff')\n",
    "\n",
    "# img = io.imread('p1.jpg')\n",
    "\n",
    "# img  = io.imread('KA.AN2.40.tiff')\n",
    "f=feature_extract(img,8)\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred label ['happy']\n"
     ]
    }
   ],
   "source": [
    "# f=extract_features(img,'hog')\n",
    "fp= pca.transform([f])\n",
    "# print(fp)\n",
    "print ('pred label', clf.predict(fp) )\n",
    "\n",
    "\n",
    "# pca = PCA(n_components=0.9)\n",
    "# fd = pca.fit_transform([f])\n",
    "# X_test1 = pca.transform(test_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
